I"Ç<p align="center">
VR interactive work, dimension variable; 2021.<br /><br />
"The most dangerous silence is noise."<br />
- Armin Wiebe <br /><br />
<iframe width="670" height="377" src="https://www.youtube.com/embed/yMyR5DKjGA0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p>

<p>What you hear cannot be undone, just as what you‚Äôve done cannot be altered. Our lives are inundated with noise, from sounds we don‚Äôt want to hear waking us up, to fake news we don‚Äôt want to pay attention to, from rumor and gossip beyond our control or ability to explicate, to misinformation designed to trigger our behaviors. To show the way noise disguised as real information impinges on our consciousness, we created a narrative intervention that follows our consciousness taking stock of our dreams and ideas as the metaphor of a train traveling through a landscape of machine-learning generated landscapes and machine-learning sorted voices. The journey is based around the arrival of the train at a location dreamt up by the machine as a cross between Rueon, France, and Yokohama, Japan, where a person we have known all our lives stands up and exits the train. That character contains the characteristics of multiple people in our lives, just as the landscape transitions between many places we know by an exploration in machine-learned latent spaces. Voices and sounds are interspersed in each location using spheres of deformations of space in VR, and exploring them triggers spatial audio that is grouped using a machine learning algorithm. Together the visual and audio exploration narrates the scenario of the arrival of the train, where our intimate character steps off without saying goodbye, while we summon up our resolve to create an environment to discredit the information source, to finally say goodbye ourselves, to turn off the sound.</p>

<p>See our paper submitted to Audiomostly Sound Interaction Conference, ‚ÄúSOUND OF(F): Exploring contextual representations of sound and music data sets using machine learning‚Äù: <a href="https://raylc.org/chairbots/Sound%20Of(f)%20AM21_02.pdf">Erol, Ozgunay &amp; LC (2021)</a>.</p>

<p align="center">
<img src="/assets/img/figs_SoundOff02.jpg" />
<img src="/assets/img/figs_SoundOff04.jpg" />
</p>

<p>In dreams, one‚Äôs life experiences are jumbled together, so that characters can represent multiple people in your life and sounds can run together without sequential order. To show one‚Äôs memories in a dream in a more contextual way, we represent environments and sounds using machine learning approaches that take into account the totality of a complex dataset. The immersive environment uses machine learning to computationally cluster sounds in thematic scenes to allow audiences to grasp the dimensions of the complexity in a dream-like scenario. We applied the t-SNE algorithm to collections of music and voice sequences to explore the way interactions in immersive space can be used to convert temporal sound data into spatial interactions. Applying it to a Virtual Reality (VR) artwork about replaying memories in a dream, we found that audiences can enrich their experience of the story without necessarily gaining an understanding of the artwork through the machine-learning generated soundscapes. This provides a method for experiencing the temporal sound sequences in an environment spatially using nonlinear exploration in VR.</p>

<p align="center">
<img src="/assets/img/figs_SoundOffimage015.png" />
<img src="/assets/img/figs_SoundOffimage017.png" />
<img src="/assets/img/figs_SoundOffimage019.png" />
</p>

:ET